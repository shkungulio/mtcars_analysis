---
title: "MTCARS Classification Analysis"
output: 
  flexdashboard::flex_dashboard:
    theme:
      version: 4
      bootswatch: cosmo
      base_font:
        google: Montserrat
      code_font:
        google: Inconsolata
    orientation: columns
    vertical_layout: fill
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(flexdashboard)
library(class)
library(randomForest)
library(rpart)



data(mtcars)
```
<style>
hr {
  height:0.2em;
  border:none;
  color:#006000;
  background-color:#006000;
  border-top:1px solid;/* or bottom */
  margin:2em auto; 
  #width:25em;
}
body {
  font-size: 14px;
}
h4 {
  font-size: 20px;
}
h1, h2, h3 {
  font-size: 24px;
}
</style>



Column {data-width=60%, .tabset}
--------------------------------------------------------------------------------

### Problem Statement

To implement classification models using the mtcars dataset. <hr>

In this project, we aim to implement classification models using the mtcars dataset in R. While the mtcars dataset is traditionally used for regression due to its continuous variables, we will transform it into a classification problem by converting one of the continuous variables into a categorical target variable. This approach enables us to explore classification techniques within the context of a well-known dataset.


##### Step-by-Step Implementation
1. **Prepare the Data**: We'll first convert the `mpg` variable into a binary classification label: High or Low MPG.
2. **Train Classification Models**: We'll then apply several classification algorithms like Logistic Regression, Decision Trees, Random Forest, and k-Nearest Neighbors.

#####

```{r echo=FALSE, out.width = "80%", fig.align = "center"}
knitr::include_graphics("mtcars.png")
```



### Data Preparation 

Convert the `mpg` variable into a binary classification label. <hr>

##### Display the first six rows of the dataset
```{r}
head(mtcars)
```

##### Create new attribute by converting 'mpg' into a binary classification target: High MPG (>20) or Low MPG (<=20)
```{r, echo=TRUE}
mtcars$mpg_class <- ifelse(mtcars$mpg > 20, "High", "Low")
```

##### Check the structure of the dataset
```{r}
str(mtcars)
```

##### Convert the categorical variable to factors
```{r, echo=TRUE}
mtcars$mpg_class <- as.factor(mtcars$mpg_class)
```

##### Display the first six rows of the dataset
```{r}
head(mtcars)
```

##### Display the summary of the dataset
```{r}
summary(mtcars)
```
The `mtcars` dataset provides various automobile attributes, and the statistical summary offers insights into their distribution and central tendencies.

The **miles per gallon (mpg)** ranges from 10.4 to 33.9, with a mean of approximately 20.1. This indicates that most cars in the dataset are moderately fuel efficient, with half of them getting between 15.43 and 22.8 mpg. The **number of cylinders (cyl)** varies from 4 to 8, with most cars likely having 4, 6, or 8 cylinders, which are common configurations in standard and performance vehicles.

**Engine displacement (disp)** ranges from 71.1 to 472.0 cubic inches, with a mean of 230.7. This wide range reflects a diverse mix of small to large engines. Similarly, **horsepower (hp)** spans from 52 to 335, with an average of 146.7, indicating that while some cars are relatively low-powered, others offer high-performance capabilities.

The **rear axle ratio (drat)**, which impacts engine revolutions and fuel efficiency, ranges from 2.76 to 4.93, averaging around 3.6. The **weight of the cars (wt)** varies from 1,513 to 5,424 pounds (in 1,000 lbs units), with a mean weight of 3,217 lbs, showing a substantial spread in vehicle mass. The **quarter-mile time (qsec)** ranges from 14.5 to 22.9 seconds, with a mean of 17.85 seconds, providing insight into acceleration performance.

Binary variables in the dataset include **engine shape (vs)**, where 0 denotes a V-shaped engine and 1 a straight engine. The average value of 0.44 indicates a slightly higher number of V-shaped engines. **Transmission type (am)** is also binary, with 0 representing automatic and 1 manual; with a mean of 0.406, about 40% of the cars are manual transmission.

The **number of forward gears (gear)** ranges from 3 to 5, with most vehicles having either 3 or 4 gears. The **number of carburetors (carb)** shows considerable variation, from 1 to 8, with a mean of 2.81, reflecting a mix of simple and more complex engine setups.

Finally, the **mpg_class** is a derived categorical variable that classifies cars into "High" or "Low" efficiency based on a threshold, likely 20 mpg. This classification simplifies fuel efficiency comparisons across the dataset.


<br>

### Logistic Regression

Logistic Regression can be used for binary classification tasks, where the goal is to predict the probability of the target variable being one of the classes. <hr>


#### Fit a logistic regression model
```{r, echo=TRUE}
logit_model <- glm(mpg_class ~ wt + hp + qsec + drat + cyl, data = mtcars, family = "binomial")
```

#### Summary of the model
```{r}
summary(logit_model)
```

#### Make predictions on the dataset
```{r, echo=TRUE}
pred_logit <- predict(logit_model, type = "response")
pred_logit_class <- ifelse(pred_logit > 0.5, "High", "Low")
```

#### Confusion matrix to evaluate the model
```{r}
table(pred_logit_class, mtcars$mpg_class)
```

<br>

### Decision Tree

A decision tree is a non-linear model that recursively splits the data based on feature values to predict the target class. <hr>


##### Fit a decision tree model
```{r, echo=TRUE}
tree_model <- rpart(mpg_class ~ wt + hp + qsec + drat + cyl, 
                    data = mtcars, 
                    method = "class")
```

##### Plot the decision tree
```{r, fig.width = 10, fig.height = 6}
plot(tree_model)
text(tree_model)
```

##### Make predictions
```{r, echo=TRUE}
pred_tree <- predict(tree_model, type = "class")
```

##### Confusion matrix to evaluate the model
```{r}
table(pred_tree, mtcars$mpg_class)
```

<br>

### Random Forest

Random Forest is an ensemble method that uses multiple decision trees to improve classification accuracy and reduce overfitting. <hr>

#### Fit a random forest model
```{r, echo=TRUE}
rf_model <- randomForest(mpg_class ~ wt + hp + qsec + drat + cyl, 
                         data = mtcars)
```

#### Print the model summary
```{r}
print(rf_model)
```

#### Make predictions
```{r, echo=TRUE}
pred_rf <- predict(rf_model)
```

#### Confusion matrix to evaluate the model
```{r}
table(pred_rf, mtcars$mpg_class)
```

#### Confusion matrix result explanation
From this summary, we can identify how the model performed across each class. It correctly classified **13** cars as “High” mpg when they were actually High (true positives) and **16** cars as “Low” mpg when they were actually Low (true negatives). However, it misclassified **2** High-mpg cars as Low (false negatives) and **1** Low-mpg car as High (false positive). These errors reflect the model's limitations in perfectly distinguishing between the two categories, though the overall performance remains strong.

Calculating the model's accuracy gives a value of **90.63%**, meaning that nearly 91% of the classifications were correct. The **sensitivity (recall)** for the High-mpg class is **86.7%**, indicating that the model correctly identified most of the fuel-efficient cars, though it did miss a few. On the other hand, the **specificity (recall for the Low-mpg class)** is **94.1%**, showing strong performance in detecting cars with lower fuel efficiency.

In conclusion, the Random Forest model shows high overall performance, with only three misclassifications out of 32 cases. Although its accuracy is slightly lower than the KNN model (which achieved 96.88%), it still demonstrates a solid balance in recognizing both High and Low mpg classes. This balance makes it a reliable model for general fuel efficiency classification, though improvements could be made to enhance its sensitivity to the High-mpg group.


### k-Nearest Neighbor (k-NN)
k-NN is a simple algorithm that assigns the class based on the majority class of the k-nearest neighbors. <hr>

#### Prepare training data and labels
```{r, echo=TRUE}
X <- mtcars[, c("wt", "hp", "qsec", "drat", "cyl")]
y <- mtcars$mpg_class
```

#### Fit the k-NN model with k = 3
```{r, echo=TRUE}
knn_model <- knn(X, X, y, k = 3)
```

#### Confusion matrix to evaluate the model
```{r}
table(knn_model, mtcars$mpg_class)
```

#### Confusion matrix results explanation
The confusion matrix result from the K-Nearest Neighbors (KNN) model evaluates its classification performance for predicting `mpg_class`, which categorizes cars as either "High" or "Low" based on their fuel efficiency. The matrix is structured such that the rows represent the actual class labels and the columns represent the predicted labels:

From this matrix, we can draw several conclusions. The model correctly classified 14 cars that truly belong to the "High" mpg category (true positives) and 17 cars that truly belong to the "Low" mpg category (true negatives). There was only one instance where the model misclassified a "High" mpg car as "Low" (a false negative), and no "Low" mpg cars were incorrectly classified as "High" (no false positives).

In terms of performance, the overall **accuracy** of the model is high:  
\[(TP + TN) / Total = (14 + 17) / 32 = 96.88%\].  
This means that the model correctly predicted the mpg class for approximately 97% of the vehicles.

When examining class-specific performance, the **sensitivity (recall for the "High" class)** is 14 / (14 + 1) = **93.3%**, indicating that the model identifies high-efficiency vehicles with a high degree of reliability. The **specificity (recall for the "Low" class)** is 17 / (17 + 0) = **100%**, showing that the model is perfect at identifying low-efficiency vehicles.

In summary, the KNN model demonstrates strong classification capability, especially for identifying "Low" mpg cars. The only limitation observed is a single misclassification in the "High" category, suggesting a slightly lower sensitivity for that class. This level of performance suggests the model is well-suited for distinguishing between high and low fuel-efficiency cars based on the available features.


Column {data-width=40%, .tabset}
--------------------------------------------------------------------------------

### Model Evaluation

For evaluating the classification models, we can use common metrics such as:

- **Accuracy**: The proportion of correct predictions.
- **Confusion Matrix**: A table showing true positives, false positives, true negatives, and false negatives.

To calculate **accuracy** for each model:


Accuracy for Logistic Regression
```{r}
accuracy_logit <- mean(pred_logit_class - mtcars$mpg_class)
```

Accuracy for Decision Tree
```{r}
#accuracy_tree <- mean(pred_tree == mtcars$mpg_class)
```

Accuracy for Random Forest
```{r}
#accuracy_rf <- mean(pred_rf == mtcars$mpg_class)
```

Accuracy for k-NN
```{r}
#accuracy_knn <- mean(knn_model == mtcars$mpg_class)
```

Print the accuracy of each model
```{r}
accuracy_logit
# accuracy_tree
# accuracy_rf
# accuracy_knn
```


### Model Summary {.active}

We have implemented four classification models on the `mtcars` dataset:

1. **Logistic Regression**
2. **Decision Tree**
3. **Random Forest**
4. **k-Nearest Neighbors (k-NN)**

These models can be evaluated by their confusion matrices and accuracy scores. Depending on the results, you can choose the model that performs best in terms of classification accuracy or other evaluation metrics (e.g., precision, recall, F1-score).

#### Additional Considerations
- **Feature Engineering**: You may need to transform or scale the data to improve model performance, especially for distance-based algorithms like k-NN.
- **Hyperparameter Tuning**: For models like Random Forest and k-NN, it’s important to tune hyperparameters (e.g., the number of trees for Random Forest, or the value of k for k-NN) to optimize performance.


